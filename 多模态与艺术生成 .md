
#起源：

openAI的 CLIP 和DALLE

https://multimodal.art/
https://blog.ml6.eu/multimodal-ai-overview-experiments-with-dall-e-clip-8734c214bee2
多模式 AI：概述 + 使用 DALL-E 和 CLIP 进行实验


#CLIP 引导的 AI 多模态与艺术结合

2021 年 1 月，OpenAI 在其 DALL-E 成像系统的连接中引入了一种名为CLIP的非常新颖的网络，它在互联网上引发了一场革命，并随之引发了人工智能艺术的爆炸式增长。

CLIP训练了4亿对图像-文本，imageNET只有120万。


CLIP 已经使用大量图像及其字幕数据集进行了训练，以找出给定文本与图像的对应程度——反之亦然，或图像与图像之间的对应关系，因为一旦图像和文本被网络读取，它们处于平等地位。它真正神奇的地方在于，经过训练后，它可以对从未遇到过的句子和图像进行评估和评分，这意味着我们可以用它逐步将任何图像转换为符合prompt – 用作目标的文本或其他图像。

要将 CLIP 投入使用，我们需要选择一个图像生成器。这几乎可以是任何东西，但神经网络是最有效的。

---

##CLIP知识

视频知识：
1）How does CLIP Text-to-image generation work
https://www.youtube.com/watch?v=-b7xKWeADHQ



最早的 CLIP 图像生成笔记本之一是Big Sleep ，由Ryan Murdock于 2021 年 1 月发布，（链接是 lucidrain 的笔记本的简化版本）它使用 BigGAN 作为图像生成器。
Big Sleep 为Katherine Crowson 的CLIP+VQGAN (z+quantize)笔记本提供了灵感，现在可用的绝大多数 CLIP 笔记本的前身。它于 2021 年 4 月发布，并在次年夏天在 YouTube上出现了西班牙语版本的colab时，它的人气爆发式增长。现在有许多原始 z+quantize notebook 的衍生产品和变体。
Crowson 还是另一种 CLIP 引导生成方法的创建者，即Guided Diffusion notebook，它使用完全不同的图像生成方式。扩散图像往往非常优雅（成功时）且清晰，轮廓清晰。这个笔记本也有许多后继者，其中一个值得注意的是 Dan Russell 的Quick Diffusion笔记本。


---


##非GAN：

###Aphantasia以及升级版本illustrip

https://www.patreon.com/eps696/posts

Based on CLIP + FFT/pixel ops from Lucent. 3D part by deKxi, based on AdaBins depth
FFT from Lucent（https://github.com/greentfrapp/lucent）

采用更直接的方法并使用图像频率
CLIP + FFT/DWT/RGB = 文本到图像/视频

aphantasia [text to image]
https://github.com/eps696/aphantasia
https://colab.research.google.com/github/eps696/aphantasia/blob/master/Aphantasia.ipynb

illustrip [text to video]
https://colab.research.google.com/github/eps696/aphantasia/blob/master/IllusTrip3D.ipynb


###CLIPDRAW

目前的方法已经通过类似 GAN的方法显示出惊人的逼真图像生成。然而，现实主义是一把双刃剑——生成逼真的渲染有很多开销，而我们通常想要的只是简单的绘图
这项工作介绍了 CLIPDraw，这是一种基于自然语言输入合成新颖图画的算法。 CLIPDraw 不需要任何培训； 而是使用预训练的 CLIP 语言图像编码器作为度量，以最大化给定描述和生成的绘图之间的相似性。 至关重要的是，CLIPDraw 对矢量笔画而不是像素图像进行操作，这是一种使绘图偏向于更简单的人类可识别形状的约束。 结果比较了 CLIPDraw 和其他通过优化合成的方法，并突出了 CLIPDraw 的各种有趣行为，例如以多种方式满足模棱两可的文本，可靠地生成不同艺术风格的绘图，以及从简单到复杂的视觉表示缩放为笔画 计数增加。
CLIPDraw 的灵感来自网络绘图和猜谜游戏Skribbl.io。基本的 CLIPDraw 循环遵循综合优化这一原则。首先，它从人工给出的描述提示和一组随机的贝塞尔曲线开始。然后，它通过梯度下降慢慢调整曲线，使绘图与给定的提示最匹配。贝塞尔曲线是计算机图形学中使用的“参数曲线”。
Frans 说：“我确实相信偏向绘画而不是写实主义会给图像带来更多的表达自由，优化贝塞尔曲线是一种有效地做到这一点的好方法，”弗兰斯说，“我个人也喜欢这种艺术风格，我认为这些绘画非常类似于艺术家会创作的作品。” 


https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb

背后的技术之一
https://people.csail.mit.edu/tzumao/diffvg/

###StyleCLIPDraw
StyleCLIPDraw: 文本到绘画合成的内容风格耦合。随着CLIP图像-文本编码器模型等技术的发布，使用机器学习生成符合给定文本描述的图像有了很大的改进；然而，目前的方法缺乏对要生成的图像风格的艺术控制。本文提出StyleCLIPDraw，在CLIPDraw文本到图画合成模型中加入了风格损失，以允许在通过文本控制内容的同时对合成图画进行艺术控制。在生成图像上执行解耦风格迁移只影响纹理，而本文提出的耦合方法能在纹理和形状中捕捉到一种风格，表明绘画风格与绘画过程本身是耦合的

https://colab.research.google.com/github/pschaldenbrand/StyleCLIPDraw/blob/master/Style_ClipDraw.ipynb


##Aleph-Image: CLIP x DAll-E（实际中 dVAE部分）

CLIP + dalle编码器d-VAE

https://colab.research.google.com/drive/1Q-TbYvASMPRMXCOQjkxxf72CXYjR_8Vp?usp=sharing




##BIGGAN系列

###BIG SLEEP

colab：
https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb?usp=sharing

最早的 CLIP 图像生成笔记本之一是Big Sleep 
BigGAN 作为图像生成器
开发者：Ryan Murdock
https://twitter.com/advadnoun


###Story2Hallucination
It's a modification of The Big Sleep to update the input text and render the transitions
uses Big Sleep to make videos from text
https://colab.research.google.com/drive/1cCo7z-HaoiUCqvPJJTjczmJl-_iWCcCl?usp=sharing


##StyleGAN系列

###stylegan_nada
CLIP-Guided Domain Adaptation of Image Generators

是否可以训练生成模型从特定域生成图像，仅由文本提示引导，而无需看到任何图像？换句话说：可以盲目训练图像生成器吗？利用大规模对比语言图像预训练 (CLIP) 模型的语义能力，我们提出了一种文本驱动的方法，该方法允许将生成模型转移到新域，而无需从这些域中收集甚至单个图像。我们表明，通过自然语言提示和几分钟的训练，我们的方法可以使生成器适应具有不同风格和形状的众多领域。值得注意的是，使用现有方法很难或完全不可能达到这些修改中的许多。我们在广泛的领域进行了广泛的实验和比较。
https://colab.research.google.com/github/rinongal/stylegan-nada/blob/main/stylegan_nada.ipynb

###StyleCLIP
StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery 

https://github.com/orpatashnik/StyleCLIP

受 StyleGAN 在各种领域生成高度逼真图像的能力的启发，最近的许多工作都集中在了解如何使用 StyleGAN 的潜在空间来操纵生成的和真实的图像。然而，发现具有语义意义的潜在操作通常需要对许多自由度进行艰苦的人工检查，或者为每个期望的操作收集带注释的图像。在这项工作中，我们探索利用最近引入的对比语言图像预训练 (CLIP) 模型的强大功能，以便为 StyleGAN 图像处理开发基于文本的界面，而无需此类手动操作。我们首先介绍了一种优化方案，该方案利用基于 CLIP 的损失来修改输入潜在向量以响应用户提供的文本提示。下一个，我们描述了一个潜在的映射器，它为给定的输入图像推断一个文本引导的潜在操作步骤，允许更快、更稳定的基于文本的操作。最后，我们提出了一种将文本提示映射到 StyleGAN 样式空间中与输入无关的方向的方法，从而实现交互式文本驱动的图像操作。

https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/optimization_playground.ipynb

https://replicate.com/orpatashnik/styleclip



## VQGAN

使用最多的生成器是海德堡大学开发的 VQGAN，它是另一种 GAN，在渲染逼真的纹理方面特别出色。
https://compvis.github.io/taming-transformers/

使用教程：
https://docs.google.com/document/d/1Lu7XPRKlNhBQjcKr8k8qRzUzbBW7kzxb5Vu72GMRn2E/edit

相关的colab资料汇集：https://www.reddit.com/user/Wiskkey/comments/p2j673/list_part_created_on_august_11_2021/

colab：

1）By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). 
https://colab.research.google.com/drive/1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN

2）PyTTI 
https://colab.research.google.com/github/pytti-tools/pytti-notebook/blob/main/pyttitools-PYTTI.ipynb
PyTTI 是一个功能丰富的工具集合，用于文本引导的 AI 辅助图像生成和动画。
生成器：VQGAN

3）Hypertron v2
https://colab.research.google.com/drive/1N4UNSbtNMd31N_gAT9rAm8ZzPh62Y5ud?usp=sharing

---

##diffusion系列

https://www.youtube.com/watch?v=344w5h24-h8
Diffusion models explained. How does OpenAI's GLIDE work?

扩散图像往往非常优雅（成功时）且清晰，轮廓清晰


1）Generates images from text prompts with CLIP guided diffusion

By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses OpenAI's 256x256 unconditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images.

https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj

2）修改版本 Quick CLIP Guided Diffusion HQ 256x256 and 512x512

Modified by Daniel Russell (https://github.com/russelldc, https://twitter.com/danielrussruss) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.


3）* Disco Diffusion  *

https://colab.research.google.com/drive/1sHfRn5Y0YKYKi1k-ifUSBFRNJ8_1sa39?usp=sharing

Disco diffusion的cheatsheet
https://botbox.dev/disco-diffusion-cheatsheet/

相关的教学视频：

Quick & easy tutorial for Disco Diffusion V4.1 Google Colab
https://www.youtube.com/watch?v=FA2MNG8D5x0

Image Generation with CLIP + Diffusion models (Disco Diffusion 4.1)
https://www.youtube.com/watch?v=Dx2G940Pao8

Disco Diffusion tutorial: 6 tips for prompts & init_image. Including using EMOJIS as prompts
https://www.youtube.com/watch?v=JDXD_-zek8k

Quick tutorial for Disco Diffusion "init_image". Let's steampunk the Mandalorian helmet
https://www.youtube.com/watch?v=dEqY8MYzjsY&list=PLc9_vneTcTGXdjoCD0b_-2x3-HqsvxCZH&index=9


Disco Diffusion v5.2 - Now with VR Mode
https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb

Disco Diffusion v5 (Turbo+Smooth) - Now with 3D animation
https://colab.research.google.com/github/zippy731/disco-diffusion-turbo/blob/main/Disco_Diffusion_v5_Turbo_%5Bw_3D_animation%5D.ipynb




4）latent diffusion

https://colab.research.google.com/github/multimodalart/latent-diffusion-notebook/blob/main/Latent_Diffusion_LAION_400M_model_text_to_image.ipynb

*Latent Diffusion LAION-400M*
CompVis released its LAION 400M trained model for its Dec/2021 "Latent Diffusion" implementation. It is a breakthrough, with its major feat being great text synthesis, beating even Dall-E 2. The model also samples images super fast. Sometimes it struggles to interpret the prompt, performing worst than other VQGAN-CLIP and Guided Diffusion methods. 

https://colab.research.google.com/drive/1x4p2PokZ3XznBn35Q5BBD6K6Zs-tot5t?usp=sharing


---

##Prompt的重要性

您无需了解任何有关生成高质量图像的底层技术。使用 CLIP 制作图形的很大一部分是想出合适的提示——在表达你想看到的内容时花一些心思，并就你希望它如何执行提供一些风格指导，可以带来无穷无尽的令人愉悦的结果。这种文字游戏被称为即时工程。

prompt与艺术的研究

https://matthewmcateer.me/blog/clip-prompt-engineering/

prompt词汇生成的图案排名 the AI powered drawing competition
https://paint.wtf/

重点排序：
https://weirdwonderfulai.art/resources/disco-diffusion-70-plus-artist-studies/

里面展示了79多个艺术家在同一个promt下展示的不同效果


例如：
1）展示了如何仅仅添加不同艺术时期的名称会导致对同一主题的描绘发生重大变化 https://imgur.com/a/VjROh95
2）对不同风格提示的广泛调查，CLIP + VQGAN keyword comparison by @kingdomakrillic https://imgur.com/a/SnSIQRu
3）prompt实验
https://docs.google.com/document/d/1qy5fdeThu7pIikulQuWpmKvYBiv9wMshIHcrBr-VldA/edit
4）Art Movements and Styles as perceived by VQGAN + Clip (Imagenet 16k, ViT-B/32)
https://imgur.com/gallery/BZzXLHY

5）Artist Studies by @remi_durant:
https://remidurant.com/artists/#
介绍：The wonderful thing about AI art is that it is allowing more people to tap into their own creativity. I've learned that one of the biggest tricks to generating great AI art is to include the names of prominent real world artists in your prompt, so I set out to explore what artists will work well, and what styles, colors, and subjects their names evoke.

6）My Tips for Prompts, Part 1. How to invent a good prompt for Disco Diffusion & VQGAN+Clip model
https://www.youtube.com/watch?v=w7VFkU1zM30

Good Names for Prompts, Part 2. Disco Diffusion 4.1 & 5.1.
https://www.youtube.com/watch?v=iE7oBiVZv1s&t=89s

Disco Diffusion tutorial: 6 tips for prompts & init_image. Including using EMOJIS as prompts


7）https://ecency.com/ai-art/@penderis/your-words-matter-unless-you

作者的其他分享：https://ecency.com/art/@penderis/generating-art-from-shower-thoughts


8）A Guide to Writing Prompts for Text-to-image AI
https://docs.google.com/document/d/1XUT2G9LmkZataHFzmuOtRXnuWBfhvXDAo8DkS--8tec/edit
---

##关于这种艺术形式的探讨

https://www.rightclicksave.com/article/clip-art-and-the-new-aesthetics-of-ai
https://ml.berkeley.edu/blog/posts/clip-art/

